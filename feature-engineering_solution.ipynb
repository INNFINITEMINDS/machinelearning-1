{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis and Feature Engineering\n",
        "\n",
        "## Loading the libraries\n",
        "\n",
        "For this module, you will need to import \n",
        "\n",
        "* `numpy` [link to documentation](https://docs.scipy.org/doc/)\n",
        "* `pandas` [link to documentation](https://pandas.pydata.org/pandas-docs/stable/)\n",
        "* `matplotlib.pyplot` [link to documentation](https://matplotlib.org/contents.html)\n",
        "* `seaborn` [link to documentation](https://seaborn.pydata.org)\n",
        "\n",
        "We will use `sklearn` as well but later and import it then. Remember to use the `?` and the `TAB` to help you with the code!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the data\n",
        "\n",
        "For this module, you will consider a simple retail dataset consisting of history of purchases done by customers from a variety of country. The dataset is not of exceptional interest but will allow us to simply illustrate some of the standard steps that can be followed when having a first look at a datset.\n",
        "\n",
        "**TASKS**\n",
        "\n",
        "Load the dataset `retail_data.csv` with `pandas`. Recall that you need to \n",
        "\n",
        "* use the `read_csv()` method\n",
        "* point to the location of the dataset (in the `data` folder)\n",
        "* determine a name under which you want to store the resulting data frame (we suggest the name `customers`)\n",
        "* specify that the `CustomerID` column is the index column using the `index_col` option\n",
        "\n",
        "Use the `head` method to display the first few lines of the dataset (you can specify how many lines)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers = pd.read_csv('data/retail_data.csv', \n",
        "                         index_col=\"CustomerID\")\n",
        "customers.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### About the dataset\n",
        "\n",
        "The dataset is based on data from an online retailer selling gifts and is based on a dataset taken from [here](https://archive.ics.uci.edu/ml/datasets/Online+Retail#).\n",
        "\n",
        "We have taken the original data set and processed it to create a 'profile' for each customer, which includes a number of features including:\n",
        "\n",
        "* `Country`: The country their purchases were made from.\n",
        "* `balance`: Amount of money spent at the store (purchases minus refunds).\n",
        "* `n_orders`: Total number of orders from the online retailer.\n",
        "* `time_between_orders`: Average time (in days) between orders.\n",
        "* `max_spent`: Most amount of money customer spent on a single order.\n",
        "\n",
        "**TASKS**\n",
        "\n",
        "* Check the dimensionality of the dataset using the `shape` attribute of the data frame.\n",
        "* You can also retrieve a few *summary statistics* using the `describe` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(customers.shape)\n",
        "customers.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring the dataset\n",
        "\n",
        "At this stage you do not really know what is going on in this dataset. \n",
        "You need to go beyond the first impression by considering simple questions like:\n",
        "\n",
        "* How many customers are you dealing with?\n",
        "* What country spends how much?\n",
        "* What has been the company's profit during the last year?\n",
        "* Are there differences between customers that return and those who don't?\n",
        "\n",
        "**TASKS**\n",
        "\n",
        "You will go through these questions and learn new tricks as you move along. The first one is easy to answer, you can use the `nunique()` method applied to the index column. You can retrieve the index column using the `index` attribute of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cca": {
          "exercise": false
        }
      },
      "outputs": [],
      "source": [
        "print(customers.index.nunique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result should match the number you obtained when inspecting the shape of the DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Types of features and One-Hot-Encoding\n",
        "\n",
        "Data can be both \n",
        "\n",
        "* on a *continuous* scale: e.g.: the amount of money spent in the store or time between purchases\n",
        "* on a *discrete* scale: e.g.: the country.\n",
        "\n",
        "Discrete variables that have a notion of ordering (for instance, a survey that asks your satisfaction on a scale of 1 to 5) are called *ordinal*. Discrete variables that cannot directly be ordered are usually referred to as *categorical* variables (eg: countries or gender).\n",
        "\n",
        "In the feature engineering step, one typically needs to pay special attention to discrete variables as many models are not equipped to handle this type of data, particularly if they are just categorical.\n",
        "\n",
        "In the case where categorical features are present, you need to represent them as numerical values. \n",
        "A standard approach to do so is the **one-hot encoding**. \n",
        "The input in one-hot encoding is the vector of discrete categorical values, and the output is a sparse matrix with 1s and 0s where each column corresponds to one possible value of the feature.\n",
        "As an example, let's consider the following trivial dataset:\n",
        "\n",
        "```\n",
        "Nick, UK\n",
        "Laura, IT\n",
        "Massimo, IT\n",
        "```\n",
        "\n",
        "In this case, there are two countries `[\"UK\", \"IT\"]`, the one-hot-encoding would correspond to the table\n",
        "\n",
        "$\n",
        "\\begin{array}{l|cc}\n",
        "& \\text{UK} & \\text{IT} \\\\\\hline\n",
        "\\text{Nick} & 1 & 0\\\\\n",
        "\\text{Laura} &0& 1\\\\\n",
        "\\text{Massimo}&0 & 1\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "**TASKS**\n",
        "\n",
        "In order to do that on the original dataset:\n",
        "\n",
        "* select the column `Country` and call it `countries`\n",
        "* using the function `get_dummies()` from `pandas`, apply the one-hot-encoding\n",
        "* use `head()` to have a look and make sure it all makes sense\n",
        "* drop the country column from the initial dataset using `drop` or `del` (so that it is not in our way when applying scaling)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "countries = customers['Country']\n",
        "countries = pd.get_dummies(countries)\n",
        "countries.head()\n",
        "\n",
        "customers = customers.drop('Country', axis = 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to do pre-processing and imputation on the continuous features, so let's keep this separate for now and join it together later.\n",
        "\n",
        "### Missing values and Imputation\n",
        "\n",
        "You may have noticed that there are missing values in the data (`NaN`).\n",
        "It's very important in general to check whether there are any and\n",
        "\n",
        "* whether these missing values are informative or not\n",
        "* whether you can replace the missing values in a sensible way or not\n",
        "\n",
        "**TASKS**\n",
        "\n",
        "First, check which column has missing values and how many.\n",
        "For this, use\n",
        "\n",
        "* the `isnull()` method applied on the data frame, this returns a dataframe similar to the original one but where every entry is just `True` or `False`\n",
        "* on the resulting dataframe, apply the `sum()` method which will count how many entries of the column are `True`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(pd.isnull(customers).sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*What do you think might be the reason why some customers have missing values as their `time_between_orders`?*\n",
        "\n",
        "In general, for columns with missing values, there are a few choices on how to handle them. \n",
        "This process is usually called *imputation*.\n",
        "\n",
        "#### Imputation \n",
        "\n",
        "There are many strategies to help with missing data and they depend on whether the missing data is numeric or categorical. Recall that you can for example\n",
        "\n",
        "* simply remove rows where there is missing data (e.g. `.dropna()` can achieve this)\n",
        "* imputing the values with a summary statistic such as mean or median or most frequent value (e.g. `Imputer` from `sklearn` module)\n",
        "* replace the values with a sensible estimate\n",
        "\n",
        "What strategy is best for you problem very much depends on the specifics of your dataset. \n",
        "\n",
        "In the current case, the missing values are exclusively found in the `time_between_orders` column, so you should have a look at these rows where this occurs to see if we can gain an understanding of what may be causing these missing values.\n",
        "\n",
        "**TASKS**\n",
        "\n",
        "* select the customers for which `time_betwee_orders` is null. For this, use `isnull` on the appropriate column and feed it as row indices to the dataframe to retrieve a subdataframe only corresponding to those customers\n",
        "* check the shape, make sure it worked!\n",
        "* have a look at the resulting dataframe, can you spot anything strange? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "nan_customers = customers[pd.isnull(customers['time_between_orders'])]\n",
        "\n",
        "# How many nan cases do we have? \n",
        "print(nan_customers.shape)\n",
        "\n",
        "# Let's have a quick look\n",
        "nan_customers.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that `n_orders` seems to often be equal to `1`. \n",
        "There is a fairly obvious interpretation for those: since they haven't yet come back, there is no \"time between orders\".\n",
        "\n",
        "**TASKS**\n",
        "\n",
        "You can count the number of time specific values of `n_orders` occur by using the `value_counts()` method applied on the series corresponding to `n_orders`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "nan_customers[\"n_orders\"].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The overwhelming majority of missing values can therefore be explained by customers that have not returned before.\n",
        "It is unclear at this point what the remaining 200 are. The dataset is of course fairly artificial so we won't discuss this in too much detail at this point. \n",
        "\n",
        "**TASKS**\n",
        "\n",
        "In this case, we decide to fill the missing values by 365 days by using the `fillna()` function (this is a **fairly reckless** decision but, again, this notebook is more focused on tools and techniques)\n",
        "\n",
        "* replace the column `time_between_orders` by the same column where the missing values are filled with value 365 using the `fillna()` function applied on the column\n",
        "* use `head()` to check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers['time_between_orders'] = customers['time_between_orders'].fillna(365)\n",
        "customers.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Removing Outliers\n",
        "\n",
        "Outliers are observation that appear extreme relative to the bulk of the data.\n",
        "Machine Learning techniques can be sensitive to outliers. \n",
        "\n",
        "Here you will see how you can get rid of them if that's what you decide to do. \n",
        "There are multiple ways to define outliers, one possibility is to consider all points that are more than `k` standard deviations (`sigma`, $\\sigma$) away from the mean (`mu`, $\\mu$) of the data.\n",
        "\n",
        "Below you can see a simple function that takes data and a number of standard deviations and filters out everything that doesn't lie in the range $[\\mu-k\\sigma, \\mu+k\\sigma]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# this function takes a Series and filters out all elements that are outside\n",
        "# the range [mu-k*sigma , mu+k*sigma]\n",
        "def remove_outliers(data, k=3):\n",
        "    mu       = data.mean() # get the mean\n",
        "    sigma    = data.std()  # get the standard deviation\n",
        "    filtered = data[(mu - k*sigma < data) & (data < mu + k*sigma)]\n",
        "    return filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**TASKS**\n",
        "\n",
        "You can `.apply()` this function to your dataframe. In case that the value is declared an outlier, its value is replace by `NaN`, keeping the structure of the `pd.DataFrame` intact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers = customers.apply(remove_outliers)\n",
        "customers.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**TASKS**\n",
        "\n",
        "Now, remove the lines with `NaN` values (that correspond to lines with outliers). For this, use the `dropna()` method on the dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers = customers.dropna()\n",
        "customers.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scaling\n",
        "\n",
        "The different numerical features have completely different scales. This becomes even more obvious when considering a boxplot of the features. You will use the `seaborn` wrapper around `matplotlib` that is great for producing clear plots when looking at data. \n",
        "Have a look [here](https://stanford.edu/~mwaskom/software/seaborn/examples/index.html) for a gallery of plots possible with `seaborn`.\n",
        "\n",
        "**TASKS**\n",
        "\n",
        "* define a figure environment with the `figure()` method of `matplotlib.pylab` (you can pass a figure size)\n",
        "* use the `boxplot` function of `seaborn` specifying the appropriate dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(17, 7))\n",
        "sns.boxplot(data=customers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that `n_orders` is defined in a much narrower space than `balance` for example. \n",
        "If you were to use the data in an unscaled form, the effect of `balance` might be disproportionnaly high and cause a Machine Learning algorithm to underperform. \n",
        "\n",
        "To account for this, it is good practice to center and scale your data, so that all the dimensions fall onto a comparable interval.\n",
        "\n",
        "**TASKS**\n",
        "\n",
        "* Define a \"scaler\" using the `StandardScaler` class imported from `sklearn.preprocessing` (you could also use the `MinMaxScaler` though the `StandardScaler` is more common)\n",
        "* Apply it on the dataframe using the `fit_transform` method \n",
        "* Define a new dataframe similar to the original one but with scaled columns (make sure you specify the `columns` and `index` of the new dataframe using the previous dataframe's `columns` and `index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialise the scaler\n",
        "scaler = StandardScaler() \n",
        "\n",
        "# Apply auto-scaling (or any other type of scaling) and cast to DataFrame \n",
        "customers = pd.DataFrame(\n",
        "                        scaler.fit_transform(customers), \n",
        "                        columns = customers.columns, \n",
        "                        index   = customers.index )\n",
        "\n",
        "# Print the first rows\n",
        "customers.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**TASKS**\n",
        "\n",
        "Replot the `boxplot` with the scaled data. Observe that now all the features have most of their \"mass\" (main part of their observed values) in the same range. Note though that scaling does not change the distribution of features and you can still observe that some features are heavily skewed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(17, 7))\n",
        "sns.boxplot(data=customers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've done all this pre-processing, it's useful to save the processed dataset for further use. \n",
        "\n",
        "**TASKS**\n",
        "\n",
        "* use the `to_csv()` method on the dataframe\n",
        "* set the name to `retail_data_postfeng.csv`\n",
        "* do the same with the `countries` dataframe, call it `retail_data_countries.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "customers.to_csv('data/retail_data_postfeng.csv')\n",
        "countries.to_csv('data/retail_data_countries.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Relationship between input features\n",
        "\n",
        "An important tool for the exploratory data analysis step is the **scatter plot**. \n",
        "\n",
        "This plot helps visualise the relationship in-between two input features. It may also give you a first indication of the Machine Learning model that could be applied and its complexity (linear vs. non-linear). \n",
        "\n",
        "Given the small number of features, you can have a look at the `pairplot` of all of the features: a grid where each pair of feature is displayed against the other. This can help seeing the correlations present in your data. \n",
        "\n",
        "* use `sns.pairplot` on the DataFrame to visualise this\n",
        "* can you interpret some of the relations that appear in the grid?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.pairplot(customers)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}