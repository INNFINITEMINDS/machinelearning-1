{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supervised Learning\n",
        "\n",
        "You will now explore data coming from the american Bureau of Transportation Statistics where they recorded (a lot of) data from flights in the US from 1987 to 2008 and analysed the causes of delays. \n",
        "We will only look at data from 2008 and a subset of around 100 000 instances. We also removed some of the columns to simplify the analysis\n",
        "\n",
        "The aim is to build a classifier that can predict whether a flight will arrive with a significant delay given the parameters at takeoff.\n",
        "\n",
        "### Loading the data\n",
        "\n",
        "As usual, start by loading `pandas`, `numpy`, `matplotlib` and `seaborn` and load the data corresponding to the file `flights08.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"data/flights08.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Getting a first look at the data\n",
        "\n",
        "Have a look at the data:\n",
        "\n",
        "* Do the attributes make sense? (see [here](http://stat-computing.org/dataexpo/2009/the-data.html) if needed)\n",
        "* What's the shape of the dataset?\n",
        "* How many missing values are present?\n",
        "* How many unique values are present per attribute? what does that tell you? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Shape of the data: {}\\n\".format(data.shape))\n",
        "print(\"Missing values? ...\\n\")\n",
        "print(data.apply(np.isnan).sum())\n",
        "\n",
        "print(\"\\nNumber of unique values?...\\n\")\n",
        "print(data.apply(pd.Series.nunique))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dealing with missing values\n",
        "\n",
        "The previous step should have shown you two things:\n",
        "\n",
        "1. some features have a **lot** of missing values (in particular those associated with Delay at departure). In the sequel we will assume that a missing value for a Delay amounts to no Delay. \n",
        "2. some feature don't have enough unique values to be interesting (which ones?) and should probably removed. \n",
        "\n",
        "Based on this:\n",
        "\n",
        "* fill the missing values associated with `*Delay` by a 0\n",
        "* remove the feature(s) that don't have enough variability\n",
        "* remove all instances that have missing values left"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "del data['Month']\n",
        "\n",
        "for col in ['CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay']:\n",
        "    data[col].fillna(0, inplace=True)\n",
        "\n",
        "data.dropna(axis=0, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracting the response\n",
        "\n",
        "Our aim is to predict whether there will be a significative delay. \n",
        "The variable that encodes the delay is `ArrDelay`. \n",
        "\n",
        "* Start by having a look at it using `distplot` from `seaborn` \n",
        "* then compute the delay threshold such that 70% of the positive delays are lower than that threshold\n",
        "* form a response vector `major_delay` being either 0 or 1 depending on whether the delay is less than or greater or equal to the threshold\n",
        "* finally remove the `ArrDelay` column from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.distplot(data['ArrDelay'])\n",
        "\n",
        "all_delays = data['ArrDelay']\n",
        "positive_delays = all_delays[all_delays > 0]\n",
        "delay_threshold = np.percentile(positive_delays, 70)\n",
        "\n",
        "# Note you can check this worked:\n",
        "print(\"Percentage higher than threshold? {}pct.\".format(\n",
        "    100 * sum(positive_delays > delay_threshold) / len(positive_delays)))\n",
        "\n",
        "major_delay = (all_delays >= delay_threshold).astype(int)\n",
        "\n",
        "del data['ArrDelay']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting the data into a training and a testing set\n",
        "\n",
        "Now that you have reasonably clean data, it's time to split into a training set to train your model(s) and a test set to test those! Sklearn has all that sorted for you, of course. \n",
        "\n",
        "Import the function `train_test_split` from `sklearn.model_selection` and check the documentation using the `?` as usual. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The key options that you are most likely to use are:\n",
        "\n",
        "* `test_size` a proportion so a number between 0 and 1, typically `0.2` or `0.3`\n",
        "* `random_state` an arbitrary integer to seed the train-test split so that your experiments are reproducible\n",
        "* `stratify` in the case of imbalanced data, you want to make sure your test set and your training set contain similar proportion of the different classes. \n",
        "\n",
        "Create `X_train`, `X_test`, `y_train`, `y_test` out of `data` and `major_delay`, use `0.3` as proportion for test and set the random state to `5175`. Specify `major_delay` as the stratifier. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data, major_delay, \n",
        "                                                    test_size=0.3, random_state=5175,\n",
        "                                                    stratify=major_delay)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision Tree Classifier (DTC)\n",
        "\n",
        "We will apply a DTC to the dataset and see how it does.\n",
        "\n",
        "### Using SkLearn's DTC\n",
        "\n",
        "The procedure above can be highly optimised making the fitting of a particular DTC very fast. Much like for the kNN, SkLearn offers the `DecisionTreeClassifier` from `sklearn.tree`. Have a look at the documentation then declare a tree with no more than 3 levels. Fit it on the training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dtc = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "dtc.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Bonus) Visualising the DTC\n",
        "\n",
        "A nice feature is to export the tree and display it using `graphviz` (http://www.graphviz.org/Download..php) \n",
        "\n",
        "* on Mac: install with `Homebrew` using `brew install graphviz`\n",
        "* on Windows: http://www.graphviz.org/Download_windows.php \n",
        "* on Linux: http://www.graphviz.org/Download..php\n",
        "\n",
        "To do this, \n",
        "\n",
        "* import `export_graphviz` from `sklearn.tree`\n",
        "* use `export_graphviz` on the tree you fitted above specifying a name for the output file like `tree.dot`\n",
        "* (see also [the documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html))\n",
        "\n",
        "To see how it looks, use graphviz: \n",
        "\n",
        "```bash\n",
        "dot -Tpng tree.dot -o tree.png\n",
        "```\n",
        "\n",
        "![](tree.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "export_graphviz(dtc, out_file='tree.dot')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assessing the quality of a DTC\n",
        "\n",
        "Using\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "```\n",
        "\n",
        "* recover the confusion matrix on the training or the test set\n",
        "* recover the classification report on the training or the test set\n",
        "* adjust the depth of the tree to get optimal results\n",
        "\n",
        "(**Bonus**) if you have the time: try to explore the parameters of the DTC, what do they mean? do they help? See also [the doc](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test_pred2 = dtc.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "print(\"The confusion matrix: \\n\")\n",
        "print(confusion_matrix(y_test, y_test_pred2))\n",
        "print(\"\\nThe classification report:\\n\")\n",
        "print(classification_report(y_test, y_test_pred2, digits=3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest Classifier\n",
        "\n",
        "The accuracy is already extremely high, this is because some of the features are \"too informative\". Let's remove a few features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "del data[\"DepDelay\"]\n",
        "del data[\"TaxiOut\"]\n",
        "del data[\"Cancelled\"]\n",
        "del data[\"Diverted\"]\n",
        "del data[\"CarrierDelay\"]\n",
        "del data[\"WeatherDelay\"]\n",
        "del data[\"NASDelay\"]\n",
        "del data[\"SecurityDelay\"]\n",
        "del data[\"LateAircraftDelay\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data, major_delay, \n",
        "                                                    test_size=0.3, random_state=5175,\n",
        "                                                    stratify=major_delay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "dtc = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "dtc.fit(X_train, y_train)\n",
        "\n",
        "y_test_pred2 = dtc.predict(X_test)\n",
        "\n",
        "print(\"The confusion matrix: \\n\")\n",
        "print(confusion_matrix(y_test, y_test_pred2))\n",
        "print(\"\\nThe classification report:\\n\")\n",
        "print(classification_report(y_test, y_test_pred2, digits=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(n_estimators=50)\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_test_pred3 = rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"The confusion matrix: \\n\")\n",
        "print(confusion_matrix(y_test, y_test_pred3))\n",
        "print(\"\\nThe classification report:\\n\")\n",
        "print(classification_report(y_test, y_test_pred3, digits=3))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}